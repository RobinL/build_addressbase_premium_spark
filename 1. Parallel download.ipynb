{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will want to execute this in docker so we don't have to include BeautifulSoup in Spark.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"download_html/os_download_links_page.htm\") as f:\n",
    "    soup = BeautifulSoup(f.read())\n",
    "\n",
    "soup.find(\"a\")\n",
    "\n",
    "el = soup.find(text = \"Number of Files:\").parent.parent\n",
    "num = el.text.replace(\"Number of Files:\", \"\").strip()\n",
    "numfiles = int(num)\n",
    "\n",
    "my_links = set()\n",
    "for a in soup.findAll(\"a\"):\n",
    "    if \"href\" in a.attrs:\n",
    "        if \"AB76DL\" in a[\"href\"]:  # This is a bit of trial and error, but it turned out that the download links all contain \"AB76DL\" as part of the URL\n",
    "            my_links.add(a[\"href\"])\n",
    "my_links = list(my_links)\n",
    "\n",
    "# Check we've found the right number of links\n",
    "if (len(my_links) != numfiles):\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fns\n",
    "from pyspark import  SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# For py 2 and 3 compataibility\n",
    "try:\n",
    "    import urllib.request as urlrequest\n",
    "except ImportError:\n",
    "    import urllib as urlrequest\n",
    "    \n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_and_put_in_s3(link):\n",
    "    \n",
    "    #Must be within function otherwise tries to pickle and send boto3 client to workers\n",
    "    import boto3\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # To do - make this work in Python 2 so that it runs on glue\n",
    "    match = re.search(\"CSV\\/(\\w{2}\\d{4}\\.zip)\", link)[1]\n",
    "    urlrequest.urlretrieve(link, \"{}\".format(match))\n",
    "    \n",
    "    s3_client.upload_file(match, \"alpha-everyone\", \"deleteathenaout/abpzips/{}\".format(match))\n",
    "    \n",
    "    # Put in s3\n",
    "    return match\n",
    "\n",
    "\n",
    "rdd = sc.parallelize(my_links[:1000], numSlices=100)  # As I understand it, if we set num slices >= num cpus in the cluster, we will parallelise across all cpus\n",
    "downloaded_files = rdd.map(download_and_put_in_s3).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
